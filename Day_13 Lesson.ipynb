{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx5t6212dmGv"
      },
      "source": [
        "# Day 13: Unsupervised Learning\n",
        "\n",
        "\n",
        "How do we find patterns in data if we know minimal amounts about the data? We are looking to group data - but without foreknowledge of what groups they belong to.\n",
        "\n",
        "Examples of Unsupervised Learning:\n",
        "\n",
        "*   Dividing customers into preselected groups to determine marketing plans.  \n",
        "*   Identifying similar songs in a spotify database (without genre labels)\n",
        "*   Trying to find members of a secret organization using nothing but their emails to and from one another.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH-qJHubbNio",
        "outputId": "228b168c-36ea-4101-9616-46726b667672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyclustertend\n",
            "  Downloading pyclustertend-1.6.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.3.3 in /usr/local/lib/python3.10/dist-packages (from pyclustertend) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from pyclustertend) (1.22.4)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyclustertend) (1.5.3)\n",
            "Collecting scikit-learn<0.25.0,>=0.24.0 (from pyclustertend)\n",
            "  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#our second Pip, a very big install.\n",
        "!pip install pyclustertend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kO5-EFImizsJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.figure_factory as ff  #Simpler than scipy's clusering approach, also has great other features!\n",
        "from sklearn import preprocessing #Gotta normalize several variables, better than doing it manually each time.\n",
        "from sklearn.preprocessing import LabelEncoder #Label encoding, also a pain to do manually.\n",
        "from sklearn.cluster import KMeans\n",
        "# from pyclustertend.hopkins import hopkins #Hopkins Clustering\n",
        "\n",
        "from mpl_toolkits import mplot3d\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeQOHi8PfSDO"
      },
      "source": [
        "## Hierarchical Clustering\n",
        "\n",
        "This proceedure is deceptively simple.\n",
        "\n",
        "What makes a good group? Well, members of a group should be similar to one another.  More similar, smaller \"distance\" between numbers. Distance between members of a group should be small.\n",
        "\n",
        "So, let's take the two closest data points, and gob them together. Track which groups are glued together and how far apart they were (in a dendrogram). Continue \"gobbing\" points together until we have 1 group. This way we can see which clusters of points are most naturally associated with one another and make a determination about appropriate groupings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DeLfq4z5diHL",
        "outputId": "7b4dc1b9-4639-473e-9965-dca343f5c2c8"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0d392a37998d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfTitanic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfTitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Time to look at data and select what seems like it might be good for grouping passengers together, can use several numerical values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic3.csv'"
          ]
        }
      ],
      "source": [
        "dfTitanic = pd.read_csv('titanic3.csv')\n",
        "dfTitanic.head() #Time to look at data and select what seems like it might be good for grouping passengers together, can use several numerical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7E14x_E9jtmL"
      },
      "outputs": [],
      "source": [
        "#Everything has to be numeric, no NA's\n",
        "labelInstance = LabelEncoder\n",
        "minMaxInstance = preprocessing.MinMaxScaler()\n",
        "dfTitanic['encSex'] = labelInstance().fit_transform(dfTitanic['sex'])\n",
        "dfTitanic['encBoat'] = labelInstance().fit_transform(dfTitanic[]) #Note that this encoding assumes a relationship between the boats in some quasilinear fashion!\n",
        "dfTitanic[['stdFare','stdAge']] = minMaxInstance.fit_transform(dfTitanic[['fare','age']])   #It is vital that you standardize/normalize everything you can, otherwise the distance metrics are dominated by things like fare price and not by other elements.  Your choice here matters.\n",
        "dfClean = dfTitanic[['pclass','survived','encSex','encBoat','stdFare','stdAge']].dropna(how='any')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NCUt4bF7kgjA"
      },
      "outputs": [],
      "source": [
        "fig = ff.create_dendrogram(dfClean[1:50])# only draw a few, we want to see the tree with some\n",
        "fig.update_layout(width=800, height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49h3N9OB8yoo"
      },
      "source": [
        "We can see from the figure above that there are multiple group types, the most distinct being passengers 0 and further right on the dengram (red) and all other passengers being another group, somewhere around distance 25 away from each other.\n",
        "\n",
        "We can take horizontal cross sections (say at 20 distance) and it will allow us to identify how many groups and what the contents of those groups are. Some canidates are at distance 25 - 2 groups, 12 -3 groups, 7 - 4 groups, etc.\n",
        "\n",
        "There are different methods of determining distance of groups, the most obvious perhaps being 'average linkage' of all points in cluster between one another, 'single linkage' (distance between two closest points), and so on. Check your textbook for variations, but these generally cover the ideas of note.\n",
        "\n",
        "Critically, because this actually measures distance, one **must** use normalized/standardized data. **Results will vary by normalization/standardization choices.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px0VXp4h9neU"
      },
      "source": [
        "## K-Means Clustering\n",
        "\n",
        "Rather than start with the data and find the number of groups (at a given distance), one could designate a number of groups that we believe to be present in the data, then search for an appropriate match.\n",
        "\n",
        "Matches are determined by randomly assigning points to a cluster.\n",
        "We then find a centroid for all points in that cluster.\n",
        "Then we reassign observations to the closest centroid, and repeat until stablized.  Note that because this starts with a random point, it is **nondeterministic** in outcome.\n",
        "\n",
        "Note that an labeled group is not guranteed to be found in this data. This is looking for unlabeled groups. For example, if I feed it every factor except for survival, it will not almost certainly not identify the group survive/not survive.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oYS8cik29mO9"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=3, n_init=1234) #selecting arbitrary random seed\n",
        "kmeans.fit(dfClean)\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter3D(dfClean['encBoat'], dfClean['stdFare'], dfClean['stdAge'], c=kmeans.labels_);\n",
        "\n",
        "ax.set_xlabel('Boat')\n",
        "ax.set_ylabel('Fare')\n",
        "ax.set_zlabel('Age')\n",
        "ax.set_box_aspect(aspect=None, zoom=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zoy2C5NVjx7"
      },
      "source": [
        "Boats were a major component of the grouping, boats are seen as contiguous, but are likely stored on different floors, etc. - this is worth a long discussion and several sketches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVbizy1SW7g4"
      },
      "source": [
        "## Association Rules\n",
        "\n",
        "We have a series of items (usually events occuring in time), and we are looking to see how often the antecedent (event before) is followed by a consequent (event after).  We will call the antecdent A and the consequent B.\n",
        "\n",
        "$P(B|A) = Confidence = \\frac{P(A and B)}{P(A)}$\n",
        "\n",
        "However, this does not mean A *causes* B, those things are distinct. For example, if B simply happens all the time, then this confidence will be very high, because $P(B) \\approx 1$.   One way to correct for this specific problem is to use the lift ratio (but doesn't solve the problem entirely)\n",
        "\n",
        "$Lift\\ Ratio = \\frac{Confidence}{P(B)}$\n",
        "\n",
        "If this lift ratio exceeds 1 it suggests a rule might be useful, at least, moreso than not using A.  However, one should be cautious about how frequently both A and B occur in the dataset, one would not want to perform this evaluation when either A or B were partiuclarly sparse.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0oGzb1lapkE"
      },
      "source": [
        "## Measuring Clustering\n",
        "\n",
        "Broadly, we would like to know if a given dataset has suitability for unsupervised clustering. We can do this with *a priori* knowledge, or by visual inspection, and then complement or approach with an autonomous measure (like a Hopikins Statistic).\n",
        "\n",
        "We also would like to know if a clustering is well-done, given that some clusters are good and others are clearly... not.  One autonomous measure is the *Silhouette coefficient*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcnRlot5a1EB"
      },
      "source": [
        "### Hopkins Statistic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DZBI6PmIbBWs"
      },
      "outputs": [],
      "source": [
        "#Generally returns 0.5 if uniformly distributed, closer to 0 means more clusters.\n",
        "#Note in large data sets natural lumps will trigger this to shrink because real data deviates from theoretical norms, larger data sets are more likely to have such lumps.\n",
        "#One might argue such lumps are indeed clusters of note, but the roughness of the surface limits the ability to find whatever true groupings we believe to be present.\n",
        "hopkins(dfClean, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhS4j_ShfcaB"
      },
      "source": [
        "## Silhoette Coefficient\n",
        "\n",
        "$s(i) = \\frac{b(i)-a(i)}{max(a(i),b(i)}$\n",
        "\n",
        "Where *a(i)* is the mean distance between point *i* and members of cluster A, and *b(i)* is the measure of mean distance to members of cluster B.\n",
        "\n",
        "Ranges from -1 to 1, 1 meaning point *i* similar to cluster A, and -1 means *i* is more similar to members of cluster B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvnL09NEgJ7d"
      },
      "source": [
        "# Exercizes\n",
        "\n",
        "1) Discuss and explain why the siloette coefficient is vunerable to choices in scaling. Try to sketch out an example verbally.\n",
        "\n",
        "2) Load a fresh dataset and make a call about the number of clusters using a hirerarical clustering method.\n",
        "\n",
        "3) Load a fresh dataset and make a call about the number of clusters using a K-means cluster.\n",
        "\n",
        "4) Construct an elbow plot for the K-means plot and discuss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UOwVtFNABj0q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}